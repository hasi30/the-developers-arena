# -*- coding: utf-8 -*-
"""developers arena month 3  .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cSG1V2xBeE7B1Jp2QHeO7fS6ZId2tlZn

customer churn classifier
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    classification_report,
    ConfusionMatrixDisplay
)
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# --------------- 1. Load & Inspect -----------------
df = pd.read_csv('telecom_churn.csv')

# --------------- 2. Prepare data -------------------
X = df.drop('Churn', axis=1)
y = df['Churn']

# Stratified split keeps churn/non‑churn balance
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# --------------- 3. Random Forest ------------------
rf = RandomForestClassifier(
    n_estimators=300,
    random_state=42,
    class_weight='balanced'  # helps with class imbalance
)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
y_proba_rf = rf.predict_proba(X_test)[:, 1]

print("==== Random Forest Metrics ====")
print(f"Accuracy : {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_proba_rf):.4f}")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix plot
ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)
plt.title("Random Forest Confusion Matrix")
plt.tight_layout()
plt.show()

# Feature importance plot
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure()
plt.bar(range(len(importances)), importances[indices], align='center')
plt.xticks(range(len(importances)), X.columns[indices], rotation=90)
plt.ylabel("Importance")
plt.title("Feature Importances — Random Forest")
plt.tight_layout()
plt.show()

# --------------- 4. XGBoost (optional) --------------
try:
    from xgboost import XGBClassifier

    xgb = XGBClassifier(
        n_estimators=300,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric='logloss',
        random_state=42
    )
    xgb.fit(X_train, y_train)

    y_pred_xgb = xgb.predict(X_test)
    y_proba_xgb = xgb.predict_proba(X_test)[:, 1]

    print("==== XGBoost Metrics ====")
    print(f"Accuracy : {accuracy_score(y_test, y_pred_xgb):.4f}")
    print(f"ROC AUC  : {roc_auc_score(y_test, y_proba_xgb):.4f}")
    print(classification_report(y_test, y_pred_xgb))

except ImportError as e:
    print("xgboost is not installed in this environment, skipping XGBoost model.")

# churn_model.py
import joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# 1) TRAIN — run once (or on a schedule) to refresh the model

def train_and_save(
        csv_path: str,
        model_path: str = "churn_rf.pkl",
        random_state: int = 42
):
    """
    Train a Random-Forest churn classifier and persist it to disk
    ----------------------------------------------------------------
    csv_path    : path to telecom_churn.csv
    model_path  : where to write the *.pkl
    """
    df = pd.read_csv(csv_path)

    # ── Split
    X = df.drop("Churn", axis=1)
    y = df["Churn"]
    X_train, _, y_train, _ = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=random_state
    )

    # ── Model
    rf = RandomForestClassifier(
        n_estimators=300,
        class_weight="balanced",
        random_state=random_state,
    ).fit(X_train, y_train)

    joblib.dump({"model": rf, "columns": X.columns.tolist()}, model_path)
    print(f"✅ Model saved → {model_path!s}  |  "
          f"{len(X_train):,} training rows • {len(X.columns)} features")


# 2) call in real-time

def load_model(model_path: str = "churn_rf.pkl"):
    """
    Returns the fitted model and its training-time column order.
    """
    bundle = joblib.load(model_path)
    return bundle["model"], bundle["columns"]


def classify_client(client_features: dict | pd.DataFrame,
                    model_path: str = "churn_rf.pkl",
                    return_proba: bool = True):
    """
    Single-call helper:
      • Accepts  {feature: value, …}  or a 1-row DataFrame
      • Autoloads the model the first time it is called
      • Returns label (0/1) and optional churn probability
    """
    # lazy-load the model once
    if not hasattr(classify_client, "_clf"):
        classify_client._clf, classify_client._cols = load_model(model_path)

    clf, cols = classify_client._clf, classify_client._cols

    # Convert to DataFrame & ensure column order
    if isinstance(client_features, dict):
        X_new = pd.DataFrame([client_features])[cols]
    else:                              # already a 1-row DataFrame
        X_new = client_features[cols]

    label = int(clf.predict(X_new)[0])
    if return_proba:
        prob = float(clf.predict_proba(X_new)[0, 1])
        return label, prob
    return label

# 3) QUICK DEMO - run this block directly for a smoke-test

if __name__ == "__main__":
    # (re)train and persist
    train_and_save("telecom_churn.csv")

    # sampler — fictitious subscriber profile
    example_client = {
        "AccountWeeks":      141,
        "ContractRenewal":     1,
        "DataPlan":            0,
        "DataUsage":        2.45,
        "CustServCalls":       3,
        "DayMins":         204.1,
        "DayCalls":           97,
        "MonthlyCharge":   55.30,
        "OverageFee":      13.50,
        "RoamMins":          3.2,
        # … include every column present in training!
    }

    label, prob = classify_client(example_client)
    print(f"Prediction → churn={label}  |  P(churn)={prob:.3f}")