# -*- coding: utf-8 -*-
"""myntra fashion (eda) developers arena m2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E8lqNOrXD828GjVCCRCg_a9azmkJUSLL

DEVELOPERS ARENA MONTH -2 TASKS

EDA
"""

import pandas as pd

# Load the dataset
file_path = "/content/Myntra Fasion Clothing.csv"
df = pd.read_csv(file_path)

# Display basic information about the dataset
df_info = df.info()
df_head = df.head()
df_shape = df.shape

df_info, df_head, df_shape

# Check for missing values
missing_values = df.isnull().sum().sort_values(ascending=False)

# Basic statistics for numeric columns
numeric_summary = df.describe()

missing_values, numeric_summary

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set(style="darkgrid")

# Top 10 Brands
top_brands = df['BrandName'].value_counts().head(10)

# Top 10 Categories
top_categories = df['Category'].value_counts().head(10)

# Top 10 Individual Categories
top_individual_categories = df['Individual_category'].value_counts().head(10)

# Plot
fig, axes = plt.subplots(3, 1, figsize=(12, 18))

# Plot for Brands
sns.barplot(x=top_brands.values, y=top_brands.index, ax=axes[0], palette='viridis')
axes[0].set_title('Top 10 Brands by Product Count')
axes[0].set_xlabel('Number of Products')
axes[0].set_ylabel('Brand')

# Plot for Categories
sns.barplot(x=top_categories.values, y=top_categories.index, ax=axes[1], palette='magma')
axes[1].set_title('Top 10 General Categories')
axes[1].set_xlabel('Number of Products')
axes[1].set_ylabel('Category')

# Plot for Individual Categories
sns.barplot(x=top_individual_categories.values, y=top_individual_categories.index, ax=axes[2], palette='coolwarm')
axes[2].set_title('Top 10 Individual Categories')
axes[2].set_xlabel('Number of Products')
axes[2].set_ylabel('Individual Category')

plt.tight_layout()
plt.show()

# Count of products by gender category
gender_distribution = df['category_by_Gender'].value_counts()

# Plotting
plt.figure(figsize=(8, 6))
sns.barplot(x=gender_distribution.index, y=gender_distribution.values, palette="pastel")
plt.title('Product Distribution by Gender Category')
plt.xlabel('Gender')
plt.ylabel('Number of Products')
plt.show()

"""TASK 2 T- TEST ON BUSINESS STRATEGY"""

# import pandas as pd

# # Load the dataset
# df = pd.read_csv("Myntra Fasion Clothing.csv")

# Drop missing values needed for comparison
df = df.dropna(subset=['BrandName', 'Ratings', 'Reviews', 'OriginalPrice (in Rs)'])

# Define brand positioning categories
in_house_brands = ['Roadster', 'HRX by Hrithik Roshan', 'HERE&NOW', 'DressBerry', 'ether', 'Moda Rapido']
external_brands = ['Nike', 'Adidas', 'Puma', 'Reebok', 'Levis', 'U.S. Polo Assn.', 'H&M', 'Tommy Hilfiger']

# Assign brand position label
df['BrandPosition'] = df['BrandName'].apply(
    lambda x: 'In-House' if x in in_house_brands else ('External' if x in external_brands else 'Other')
)

# Filter to only In-House and External
df_filtered = df[df['BrandPosition'].isin(['In-House', 'External'])]

# Group by brand position and get average metrics
grouped = df_filtered.groupby('BrandPosition').agg({
    'Ratings': 'mean',
    'OriginalPrice (in Rs)': 'mean',
    'Reviews': 'mean',
    'BrandName': 'count'
}).rename(columns={'BrandName': 'Product Count'})

print(grouped)

from scipy.stats import ttest_ind


metrics = ['Ratings', 'OriginalPrice (in Rs)', 'Reviews']
titles = ['Average Ratings by Brand Position', 'Average Price by Brand Position (in Rs)', 'Average Reviews by Brand Position']

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, metric in enumerate(metrics):
    sns.barplot(data=df_filtered, x='BrandPosition', y=metric, ax=axes[i], palette='pastel', ci='sd')
    axes[i].set_title(titles[i])
    axes[i].set_xlabel('')
    axes[i].set_ylabel(metric)

plt.tight_layout()
plt.show()

# Statistical Testing: Ratings, Price, and Reviews
ttest_results = {
    'Ratings': ttest_ind(
       df_filtered[df_filtered['BrandPosition'] == 'In-House']['Ratings'],
        df_filtered[df_filtered['BrandPosition'] == 'External']['Ratings'],
        equal_var=False
    ),
    'Price': ttest_ind(
       df_filtered[df_filtered['BrandPosition'] == 'In-House']['OriginalPrice (in Rs)'],
       df_filtered[df_filtered['BrandPosition'] == 'External']['OriginalPrice (in Rs)'],
        equal_var=False
    ),
    'Reviews': ttest_ind(
       df_filtered[df_filtered['BrandPosition'] == 'In-House']['Reviews'],
        df_filtered[df_filtered['BrandPosition'] == 'External']['Reviews'],
        equal_var=False
    )
}

# Format t-test results
ttest_results = {k: {'t-stat': v.statistic, 'p-value': v.pvalue} for k, v in ttest_results.items()}
ttest_results

"""TASK - 3 PREPROCESSING AND MODEL FITTING"""

print(df.columns)

# Rename for consistency
df = df.rename(columns={
    'BrandName': 'brand',
    'Category': 'category',
    'Individual_category': 'sub_category',
    'category_by_Gender': 'gender_category',
    'DiscountOffer': 'discount_offer',
    'OriginalPrice (in Rs)': 'original_price'
})

# Keep only required columns
df = df[['brand', 'category', 'sub_category', 'gender_category', 'discount_offer', 'original_price']]

# Remove '%' and convert to float
df['discount_offer'] = df['discount_offer'].str.replace('%', '').str.extract('(\d+)').astype(float) / 100

# List of selected columns
selected_columns = [
    'brand',
    'category',
    'sub_category',
    'gender_category',
    'discount_offer',
    'original_price'
]

# Check for missing values
missing_values = df[selected_columns].isnull().sum()

# Display missing value counts
print("Missing values in selected features:\n")
print(missing_values)

# Fill missing values with the mean
df['discount_offer'].fillna(df['discount_offer'].mean(), inplace=True)

print("Missing values in discount_offer:", df['discount_offer'].isnull().sum())

df_encoded = pd.get_dummies(df, columns=['brand', 'category', 'sub_category', 'gender_category'], drop_first=True)

X = df_encoded.drop('original_price', axis=1)
y = df_encoded['original_price']

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("R² Score:", r2_score(y_test, y_pred))
# Calculate MSE
mse = mean_squared_error(y_test, y_pred)
# Calculate RMSE by taking the square root of MSE
rmse = np.sqrt(mse)
print("RMSE:", rmse)



"""USING CROSS VALIDATION"""

from sklearn.model_selection import cross_val_score

mse_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

# Calculate RMSE from MSE scores
rmse_scores = np.sqrt(mse_scores)

print("Cross-validated RMSE scores:", rmse_scores)
print("Average cross-validated RMSE:", rmse_scores.mean())
print("Standard deviation of cross-validated RMSE:", rmse_scores.std())

"""TASK-4 FINETUNNING THE MODEL"""

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

# Define a range of alpha values to test
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}

# Create a Lasso model
lasso = Lasso()

# Use GridSearchCV to find the best alpha
grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X, y)

# Get the best alpha and the best model
best_alpha = grid_search.best_params_['alpha']
best_lasso_model = grid_search.best_estimator_

print("Best alpha for Lasso:", best_alpha)

# Evaluate the best model
y_pred_lasso = best_lasso_model.predict(X_test)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
print("RMSE with best Lasso model:", rmse_lasso)

# Re-calculate initial Linear Regression metrics for comparison
y_pred_linear = model.predict(X_test)
r2_linear = r2_score(y_test, y_pred_linear)
rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))

print("--- Initial Linear Regression Accuracy ---")
print("R² Score:", r2_linear)
print("RMSE:", rmse_linear)
print("-" * 30)

# Use the results from your GridSearchCV for the Lasso model
best_alpha = grid_search.best_params_['alpha']
best_lasso_model = grid_search.best_estimator_

# Predictions for the best Lasso model on the test set
y_pred_lasso = best_lasso_model.predict(X_test)

# Evaluation for the best Lasso model
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
r2_lasso = r2_score(y_test, y_pred_lasso) # Calculate R² for Lasso

print("--- Tuned Lasso Model (alpha={}) Accuracy ---".format(best_alpha))
print("R² Score:", r2_lasso)
print("RMSE:", rmse_lasso)
print("-" * 30)

# Optional: Print a comparison statement
if rmse_lasso < rmse_linear:
    print("The tuned Lasso model shows improved accuracy with a lower RMSE.")
else:
    print("The tuned Lasso model did not significantly improve accuracy (based on RMSE).")